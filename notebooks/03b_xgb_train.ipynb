{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520c5009-4e5a-441f-a219-41e8976258ec",
   "metadata": {},
   "source": [
    "# XGBoost - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900674fd-ad21-4067-8be2-ada67a65c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28ef0c-48d2-4cb6-ba6d-a788719006d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"../data\")\n",
    "MODEL_DIR = pathlib.Path(\"../models\")\n",
    "\n",
    "ARTIFACTS = pathlib.Path(DATA_DIR / \"artifacts/xgb\")\n",
    "MODELS = pathlib.Path(MODEL_DIR / \"xgb\")\n",
    "\n",
    "OUT = MODELS\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daccf0e-00fc-4c03-9090-98bd3dcd19c6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef526174-a3b7-4a06-a1eb-7479904cf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet(ARTIFACTS / \"X.parquet\")\n",
    "y = pd.read_parquet(ARTIFACTS / \"y.parquet\")[\"label_id\"]\n",
    "metadata = pd.read_parquet(ARTIFACTS / \"metadata.parquet\")\n",
    "\n",
    "with open(ARTIFACTS / \"splits.json\") as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "label_to_id = splits[\"label_to_id\"]\n",
    "id_to_label = {int(k): v for k, v in splits[\"id_to_label\"].items()}\n",
    "classes = np.array(sorted(id_to_label.keys()))  # numeric class ids\n",
    "class_names = [id_to_label[c] for c in classes]\n",
    "num_class = len(classes)\n",
    "\n",
    "# Indexes are stored as pairs (subject, experiment). Convert back to indexers.\n",
    "idx_train = pd.MultiIndex.from_tuples(splits[\"train_idx\"], names=[\"subject\",\"experiment\"])\n",
    "idx_val   = pd.MultiIndex.from_tuples(splits[\"val_idx\"],   names=[\"subject\",\"experiment\"])\n",
    "idx_test  = pd.MultiIndex.from_tuples(splits[\"test_idx\"],  names=[\"subject\",\"experiment\"])\n",
    "\n",
    "X_train, y_train = X.loc[idx_train], y.loc[idx_train]\n",
    "X_val,   y_val   = X.loc[idx_val],   y.loc[idx_val]\n",
    "X_test,  y_test  = X.loc[idx_test],  y.loc[idx_test]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \" y_train:\", y_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape,   \" y_val:  \", y_val.shape)\n",
    "print(\"  X_test: \", X_test.shape,  \" y_test: \", y_test.shape)\n",
    "print(\"Class map:\", id_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a41d7-95cd-442e-bbdd-e34ec449c828",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912768fd-553c-4278-8ab2-374a59cfea28",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "\n",
    "We will retrain on different splits, lets standarize the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2a139-72d1-4258-b868-aab744d3eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "def train_xgb(X_train, y_train, X_val, y_val, parameters=None):\n",
    "    \"\"\"\n",
    "    Minimal, explicit training helper. Expects all required keys to exist in `parameters`.\n",
    "    \"\"\"\n",
    "    params = parameters  # use as-is\n",
    "\n",
    "    # Instantiate with explicit fields from the dict\n",
    "    xgb_clf = XGBClassifier(\n",
    "        objective=params[\"objective\"],\n",
    "        num_class=params[\"num_class\"],\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        min_child_weight=params[\"min_child_weight\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        reg_lambda=params[\"reg_lambda\"],\n",
    "        reg_alpha=params[\"reg_alpha\"],\n",
    "        tree_method=params[\"tree_method\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "        n_jobs=params[\"n_jobs\"],\n",
    "        eval_metric=params[\"eval_metric\"],\n",
    "    )\n",
    "\n",
    "    # Fit with explicit fit-args from the same dict\n",
    "    xgb_clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        early_stopping_rounds=params[\"early_stopping_rounds\"],\n",
    "        verbose=params[\"verbose\"],\n",
    "    )\n",
    "    return xgb_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd475a-a2d1-4d36-87bb-59e6afac4198",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee7deb-5583-48f8-b504-f88f91d58852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": num_class,\n",
    "    \"n_estimators\": 2000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 2.0,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": seed,\n",
    "    \"n_jobs\": -1,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"verbose\": 50,\n",
    "}\n",
    "\n",
    "xgb_clf = train_xgb(X_train, y_train, X_val, y_val, parameters=params)\n",
    "\n",
    "best_iter = getattr(xgb_clf, \"best_iteration\", None)\n",
    "print(\"Best iteration:\", best_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9b8a2-07ce-47e8-8142-777a8c6c1106",
   "metadata": {},
   "source": [
    "### Check early stop and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b99d2-3ee3-4d95-81ca-3a41a1ce1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history from the fitted model\n",
    "history = xgb_clf.evals_result()\n",
    "# XGBoost names the eval sets as validation_0, validation_1 in sklearn API\n",
    "train_key, val_key = \"validation_0\", \"validation_1\"\n",
    "\n",
    "# Safety checks in case keys/metric names differ\n",
    "if train_key not in history or val_key not in history:\n",
    "    print(\"Could not find expected eval keys in evals_result(). Available keys:\", list(history.keys()))\n",
    "else:\n",
    "    # Find the metric name (should be 'mlogloss'); fall back to the first metric found\n",
    "    metrics = list(history[train_key].keys())\n",
    "    metric = \"mlogloss\" if \"mlogloss\" in metrics else metrics[0]\n",
    "\n",
    "    train_curve = history[train_key][metric]\n",
    "    val_curve   = history[val_key][metric]\n",
    "\n",
    "    rounds = np.arange(1, len(train_curve) + 1)\n",
    "\n",
    "    # Best iteration: prefer model attribute; otherwise, argmin of validation curve\n",
    "    best_iter = getattr(xgb_clf, \"best_iteration\", None)\n",
    "    if best_iter is None or best_iter < 0:\n",
    "        best_iter = int(np.argmin(val_curve))\n",
    "    best_round = best_iter + 1  # 1-based for display\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(rounds, train_curve, label=f\"Train ({metric})\")\n",
    "    plt.plot(rounds, val_curve,   label=f\"Validation ({metric})\")\n",
    "\n",
    "    # Marker at best validation point\n",
    "    plt.axvline(best_round, linestyle=\"--\")\n",
    "    plt.scatter([best_round], [val_curve[best_iter]], zorder=3)\n",
    "\n",
    "    plt.title(f\"Learning Curve: {metric} vs. iteration\")\n",
    "    plt.xlabel(\"Boosting round\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best iteration (0-based): {best_iter}\")\n",
    "    print(f\"Best {metric} (validation): {val_curve[best_iter]:.6f} at round {best_round}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c28aeb-25a9-4c3b-b02d-3bc8a628211d",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee039c-071a-42ce-ac68-1995a61efd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597e42b-1530-4d8a-b6b1-48c5adb50b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VALIDATION ---\n",
    "proba_val = xgb_clf.predict_proba(X_val)\n",
    "pred_val  = proba_val.argmax(axis=1)\n",
    "\n",
    "print(\"Validation classification report\")\n",
    "print(classification_report(y_val, pred_val, target_names=class_names, digits=3))\n",
    "\n",
    "cm_val = confusion_matrix(y_val, pred_val, labels=np.arange(len(class_names)))\n",
    "ConfusionMatrixDisplay(cm_val, display_labels=class_names).plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c5133-2d39-4f2b-ba6c-b365f091544b",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbec6fd-122a-4a8f-aeb2-58b18a68fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST (only after you're happy with tuning) ---\n",
    "proba_test = xgb_clf.predict_proba(X_test)\n",
    "pred_test  = proba_test.argmax(axis=1)\n",
    "\n",
    "print(\"Test classification report\")\n",
    "print(classification_report(y_test, pred_test, target_names=class_names, digits=3))\n",
    "\n",
    "cm_test = confusion_matrix(y_test, pred_test, labels=np.arange(len(class_names)))\n",
    "ConfusionMatrixDisplay(cm_test, display_labels=class_names).plot(cmap=\"Greens\", values_format=\"d\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae3c8b-f55c-4f88-9e4f-bb006724f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(y_test, pred_test, labels=np.arange(len(class_names)))\n",
    "cm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831689a-9514-4e51-b2d4-4f363cf4ff4b",
   "metadata": {},
   "source": [
    "### Collect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96239d-22ec-49f1-a5aa-47c433fe6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_xgb_split(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Build metrics for model predictions.\n",
    "    \"\"\"\n",
    "    labels=np.arange(len(class_names))\n",
    "\n",
    "    # Reports\n",
    "    rep_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    rep_text = classification_report(y_true, y_pred, target_names=class_names, digits=3)\n",
    "\n",
    "    # Confusion matrix (use provided label order to align with class_names)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # Build details block in your schema\n",
    "    details = {\n",
    "        \"macro\": {\n",
    "            \"accuracy\": rep_dict[\"accuracy\"],\n",
    "            \"f1\": rep_dict[\"macro avg\"][\"f1-score\"],\n",
    "            \"precision\": rep_dict[\"macro avg\"][\"precision\"],\n",
    "            \"recall\": rep_dict[\"macro avg\"][\"recall\"],\n",
    "        },\n",
    "        \"weighted\": {\n",
    "            \"f1\": rep_dict[\"weighted avg\"][\"f1-score\"],\n",
    "            \"precision\": rep_dict[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": rep_dict[\"weighted avg\"][\"recall\"],\n",
    "        },\n",
    "        \"per_class\": {\n",
    "            cls: {\n",
    "                \"f1\":        rep_dict[cls][\"f1-score\"],\n",
    "                \"precision\": rep_dict[cls][\"precision\"],\n",
    "                \"recall\":    rep_dict[cls][\"recall\"],\n",
    "            }\n",
    "            for cls in class_names\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": details[\"macro\"][\"accuracy\"],\n",
    "        \"f1_macro\": details[\"macro\"][\"f1\"],\n",
    "        \"f1_weighted\": details[\"weighted\"][\"f1\"],\n",
    "        \"report\": rep_text,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"details\": details,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3f993-c121-4617-86fa-fadd0742b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = eval_xgb_split(y_val, pred_val, class_names)\n",
    "test_metrics = eval_xgb_split(y_test, pred_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a35e0-d11e-4be4-8bbb-71819552ee21",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e34ad1-ae0b-484a-bea4-47c29621f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance \n",
    "importances = pd.Series(xgb_clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"Top 20 features:\\n\", importances.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103b508-c37a-449a-9209-d686f9fb2f2f",
   "metadata": {},
   "source": [
    "## Feature Prunning\n",
    "\n",
    "Check how the model performs after removing less important features.\n",
    "The goal is to see how lean and performant (computational wise) the inference can get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67bd8c-b39e-44f7-a261-dc2d572b5fa9",
   "metadata": {},
   "source": [
    "### Prunned Training\n",
    "\n",
    "Train models progressively removing the least important features.\n",
    "Feature importance is based on the training with all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10906b0-a81e-421b-a5fc-0ca46fd59506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Build pruning order from the CURRENT model's importances ---\n",
    "\n",
    "# We'll drop least-important first, keeping the most important for last.\n",
    "base_importances = pd.Series(\n",
    "    xgb_clf.feature_importances_, index=X_train.columns, dtype=float\n",
    ").fillna(0.0)\n",
    "\n",
    "# order to DROP (ascending importance)\n",
    "drop_order = base_importances.sort_values(ascending=True).index.tolist()\n",
    "\n",
    "total_feats = len(X_train.columns)\n",
    "start_feats = total_feats\n",
    "step = 10\n",
    "min_keep = 1 \n",
    "\n",
    "print(f\"Total features: {total_feats}. Pruning by {step}s, down to {min_keep} kept.\")\n",
    "\n",
    "# --- Sweep: ignore first k features (least important), retrain, evaluate ---\n",
    "rows = []\n",
    "kept_counts = list(range(start_feats, min_keep - 1, -step))\n",
    "# ensure last point is exactly min_keep\n",
    "if kept_counts[-1] != min_keep:\n",
    "    kept_counts.append(min_keep)\n",
    "\n",
    "for keep_n in kept_counts:\n",
    "    # how many to drop\n",
    "    drop_n = max(0, total_feats - keep_n)\n",
    "    ignored_features = drop_order[:drop_n]\n",
    "\n",
    "    print(f\"\\n=== Keep {keep_n} features (drop {drop_n}) ===\")\n",
    "    X_train_pruned = X_train.drop(columns=ignored_features, errors=\"ignore\")\n",
    "    X_val_pruned = X_val.drop(columns=ignored_features, errors=\"ignore\")\n",
    "    xgb_pruned = train_xgb(\n",
    "        X_train_pruned, y_train,\n",
    "        X_val_pruned, y_val,\n",
    "        parameters=params,\n",
    "    )\n",
    "\n",
    "    # --- Evaluate on TEST ---\n",
    "    Xte_eval = X_test.drop(columns=ignored_features, errors=\"ignore\")\n",
    "    y_pred = xgb_pruned.predict(Xte_eval)\n",
    "\n",
    "    cr = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "    row = {\n",
    "        \"kept_features\": keep_n,\n",
    "        \"dropped_features\": drop_n,\n",
    "        \"best_iteration\": int(getattr(xgb_pruned, \"best_iteration\", -1) or -1),\n",
    "        \"accuracy\": cr[\"accuracy\"],\n",
    "        \"macro_precision\": cr[\"macro avg\"][\"precision\"],\n",
    "        \"macro_recall\": cr[\"macro avg\"][\"recall\"],\n",
    "        \"macro_f1\": cr[\"macro avg\"][\"f1-score\"],\n",
    "        \"weighted_precision\": cr[\"weighted avg\"][\"precision\"],\n",
    "        \"weighted_recall\": cr[\"weighted avg\"][\"recall\"],\n",
    "        \"weighted_f1\": cr[\"weighted avg\"][\"f1-score\"],\n",
    "    }\n",
    "    for cls in class_names:\n",
    "        if cls in cr:\n",
    "            row[f\"recall_{cls}\"] = cr[cls][\"recall\"]\n",
    "            row[f\"f1_{cls}\"] = cr[cls][\"f1-score\"]\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89529c89-4392-48ae-a88a-dda4c4a9b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final tidy DataFrame for plotting\n",
    "prune_results_df = pd.DataFrame(rows).sort_values(\"kept_features\", ascending=False).reset_index(drop=True)\n",
    "prune_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6509d83-9940-442c-a165-c4077c6a1d1c",
   "metadata": {},
   "source": [
    "### Plot Prunning Results\n",
    "\n",
    "To check the affect of prunning on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc3610-62e9-4824-9997-e6f176f795af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "metrics_to_plot = [\"accuracy\", \"macro_precision\", \"macro_recall\", \"macro_f1\"]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for m in metrics_to_plot:\n",
    "    plt.plot(\n",
    "        prune_results_df[\"kept_features\"],\n",
    "        prune_results_df[m],\n",
    "        label=m.replace(\"_\", \" \").title()\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Kept Features\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Feature Pruning Impact on Performance\")\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Major x ticks every 100 (with labels)\n",
    "ax.set_xticks(range(0, prune_results_df[\"kept_features\"].max()+1, 100))\n",
    "\n",
    "# Minor x ticks every 50 (grid only, no labels)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(50))\n",
    "\n",
    "# Grid: both x and y\n",
    "ax.grid(True, which=\"major\", axis=\"both\")\n",
    "ax.grid(True, which=\"minor\", axis=\"x\")  # vertical minor gridlines\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eef10-5451-4fad-b287-a7369e69d434",
   "metadata": {},
   "source": [
    "## Noise Adding\n",
    "\n",
    "Add noise to the Test Dataset to check model robustness against noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d74678-8532-4e94-b2ea-df1005836cef",
   "metadata": {},
   "source": [
    "### Noisy Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1ae43-b31a-464f-967f-731f5f207afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Config ---\n",
    "# If you pruned features earlier, list them here so test columns match the model.\n",
    "ignored_features = []  # e.g., from your pruning choice; keep [] if using all\n",
    "\n",
    "# Columns actually used by the trained model\n",
    "cols_used = list(getattr(xgb_clf, \"feature_names_in_\", X_test.columns))\n",
    "# Ensure we drop any ignored features\n",
    "cols_used = [c for c in cols_used if c not in set(ignored_features)]\n",
    "\n",
    "# Base test matrix aligned to the model\n",
    "Xte_base = X_test[cols_used].copy()\n",
    "\n",
    "# Per-feature std (on the clean test set) for scaling noise\n",
    "feat_std = Xte_base.std(axis=0, ddof=0).replace(0, 1e-12)  # guard zero-std\n",
    "\n",
    "# Noise levels: 0%, 2%, ..., 20%\n",
    "alphas = [a / 100.0 for a in range(0, 51, 2)]\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "rows = []\n",
    "for alpha in alphas:\n",
    "    # Gaussian noise with sigma = alpha * std(feature)\n",
    "    noise = rng.normal(loc=0.0, scale=(alpha * feat_std).to_numpy(), size=Xte_base.shape)\n",
    "    Xte_noisy = Xte_base.to_numpy() + noise\n",
    "\n",
    "    # Predict with the existing model\n",
    "    y_pred = xgb_clf.predict(Xte_noisy)\n",
    "\n",
    "    # Collect metrics (like pruning loop)\n",
    "    cr = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "    row = {\n",
    "        \"noise_alpha\": alpha,                  # 0.00 .. 0.20\n",
    "        \"noise_percent\": int(alpha * 100),     # 0 .. 20 (for nicer x-axis later)\n",
    "        \"accuracy\": cr[\"accuracy\"],\n",
    "        \"macro_precision\": cr[\"macro avg\"][\"precision\"],\n",
    "        \"macro_recall\": cr[\"macro avg\"][\"recall\"],\n",
    "        \"macro_f1\": cr[\"macro avg\"][\"f1-score\"],\n",
    "        \"weighted_precision\": cr[\"weighted avg\"][\"precision\"],\n",
    "        \"weighted_recall\": cr[\"weighted avg\"][\"recall\"],\n",
    "        \"weighted_f1\": cr[\"weighted avg\"][\"f1-score\"],\n",
    "    }\n",
    "    for cls in class_names:\n",
    "        if cls in cr:\n",
    "            row[f\"recall_{cls}\"] = cr[cls][\"recall\"]\n",
    "            row[f\"f1_{cls}\"] = cr[cls][\"f1-score\"]\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "noise_results_df = pd.DataFrame(rows).sort_values(\"noise_alpha\").reset_index(drop=True)\n",
    "noise_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26de97-3ce7-4db3-9187-20cf086a1b13",
   "metadata": {},
   "source": [
    "### Plot Noise Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931bbd0-91f3-4700-92a8-54660c1de8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "metrics_to_plot = [\"accuracy\", \"macro_precision\", \"macro_recall\", \"macro_f1\"]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for m in metrics_to_plot:\n",
    "    plt.plot(\n",
    "        noise_results_df[\"noise_percent\"],\n",
    "        noise_results_df[m],\n",
    "        label=m.replace(\"_\", \" \").title()\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Noise Percent\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Noise Impact on Performance\")\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Grid: both x and y\n",
    "ax.grid(True, which=\"major\", axis=\"both\")\n",
    "ax.grid(True, which=\"minor\", axis=\"x\")  # vertical minor gridlines\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6003dd7-b62b-4b98-bb93-535f5d0b17c3",
   "metadata": {},
   "source": [
    "## Leave One Subject Out approach\n",
    "Instead of infolds, lets use LOSO to check if the model still holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b7dd9-5b31-46be-b4a8-6434794ed8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5edde52-0d76-429f-aba7-3bd1e0ff6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = seed\n",
    "subjects_all = X.index.get_level_values(\"subject\").unique().tolist()\n",
    "C = len(class_names)\n",
    "\n",
    "fold_rows = []\n",
    "cm_sum = np.zeros((C, C), dtype=int)\n",
    "\n",
    "for held in subjects_all:\n",
    "    # Split by subject\n",
    "    mask_test = (X.index.get_level_values(\"subject\") == held)\n",
    "    X_tr_full, y_tr_full = X.loc[~mask_test], y.loc[~mask_test]\n",
    "    X_te, y_te = X.loc[mask_test], y.loc[mask_test]\n",
    "    \n",
    "    # Make a validation split from the training subjects (grouped by subject to avoid leakage)\n",
    "    tr_subjects = X_tr_full.index.get_level_values(\"subject\")\n",
    "    gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=rng)\n",
    "    tr_idx, val_idx = next(gss.split(X_tr_full, y_tr_full, groups=tr_subjects))\n",
    "    X_tr, y_tr = X_tr_full.iloc[tr_idx], y_tr_full.iloc[tr_idx]\n",
    "    X_val, y_val = X_tr_full.iloc[val_idx], y_tr_full.iloc[val_idx]\n",
    "\n",
    "    # Model\n",
    "    xgb_loso = train_xgb(\n",
    "        X_tr, y_tr,\n",
    "        X_val, y_val,\n",
    "        parameters=params,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Evaluate on the held-out subject\n",
    "    proba = xgb_loso.predict_proba(X_te)\n",
    "    pred  = proba.argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_te, pred)\n",
    "    cm  = confusion_matrix(y_te, pred, labels=np.arange(C))\n",
    "    cm_sum += cm\n",
    "\n",
    "    fold_rows.append({\n",
    "        \"held_out_subject\": held,\n",
    "        \"n_test\": int(len(y_te)),\n",
    "        \"accuracy\": float(acc),\n",
    "    })\n",
    "\n",
    "loso_df = pd.DataFrame(fold_rows).sort_values(\"held_out_subject\").reset_index(drop=True)\n",
    "loso_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f437d-cb89-4afe-b379-9c1b1fa763a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOSO-CV summary over\", len(loso_df), \"folds\")\n",
    "print(loso_df[[\"held_out_subject\",\"n_test\",\"accuracy\"]])\n",
    "\n",
    "print(\"\\nMeans ± std:\")\n",
    "for col in [\"accuracy\"]:\n",
    "    print(f\"{col}: {loso_df[col].mean():.3f} ± {loso_df[col].std():.3f}\")\n",
    "\n",
    "# Pooled confusion matrix (sum over folds)\n",
    "disp = ConfusionMatrixDisplay(cm_sum, display_labels=class_names)\n",
    "disp.plot(cmap=\"Purples\", values_format=\"d\")\n",
    "plt.title(\"LOSO-CV Pooled Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47880a-1cd9-43bd-85fb-a578e40dcda8",
   "metadata": {},
   "source": [
    "## Persist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7938f73-a0a7-4d8a-884b-c17f50a0880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, joblib, time\n",
    "\n",
    "xgb_final = xgb_clf\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(xgb_final, OUT / \"xgb_model.joblib\")\n",
    "importances.head(200).to_csv(OUT / \"top_features.csv\")  # quick peek\n",
    "\n",
    "params = xgb_final.get_params()\n",
    "params[\"best_iteration\"] = int(getattr(xgb_final, \"best_iteration\", -1) or -1)\n",
    "params[\"n_features\"] = int(X.shape[1])\n",
    "\n",
    "summary = {\n",
    "    \"model\": \"XGBoost\",\n",
    "    \"params\": params,\n",
    "    \"train_time\": {},\n",
    "    \"val\": {k: (v if k != \"report\" else None) for k, v in val_metrics.items()},\n",
    "    \"test\": {k: (v if k != \"report\" else None) for k, v in test_metrics.items()},\n",
    "    \"class_names\": class_names,\n",
    "}\n",
    "\n",
    "with open(OUT / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Saved model + metadata to:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989aef7-844e-43bf-a335-43f2831304a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ddfc8-0655-4b67-9e30-777467778bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de1cd3-6e55-40ab-a71f-9db20e361535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28250dc-e07e-49cf-b8a6-41bc082d1a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d02c87-e27d-4573-9e74-5bae27106b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fall-detection)",
   "language": "python",
   "name": "fall-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
