{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1a35b2-3811-49ab-ad15-cbea53be819b",
   "metadata": {},
   "source": [
    "# Model Compare\n",
    "\n",
    "Comparing the results of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82add6a1-34f0-4a3e-b457-5f58308290ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324648a-8e08-4420-9965-eeafed6b0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"../models\")\n",
    "\n",
    "XGBOOST_RESULTS = MODEL_DIR / \"xgb\"\n",
    "MINIROCKET_RESULTS = MODEL_DIR / \"minirocket\" / \"baseline\"\n",
    "\n",
    "METRIC_FILE = \"metrics.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce516a-064a-4f73-8b4e-d9a46d136bf5",
   "metadata": {},
   "source": [
    "## Load Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1389f2-abe5-4a8d-a7d2-502e92f1f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load JSONs\n",
    "with open(XGBOOST_RESULTS / METRIC_FILE, \"r\") as f:\n",
    "    xgb_metrics = json.load(f)\n",
    "\n",
    "with open(MINIROCKET_RESULTS / METRIC_FILE, \"r\") as f:\n",
    "    minirocket_metrics = json.load(f)\n",
    "\n",
    "# Keep them in a dict for easy access later\n",
    "all_metrics = {\n",
    "    \"xgboost\": xgb_metrics,\n",
    "    \"minirocket\": minirocket_metrics,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a59c8-6adc-4847-b4a9-507c67cee074",
   "metadata": {},
   "source": [
    "## Build Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6d88a-777a-4696-97d9-a53ba856c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Grab metrics from the nested \"details\" blocks only, for clean plotting.\n",
    "# By default we use the \"test\" split; switch to \"val\" if you want.\n",
    "\n",
    "def build_df_overall(all_metrics, split=\"test\"):\n",
    "    rows = []\n",
    "    for model, m in all_metrics.items():\n",
    "        d = m.get(split, {}).get(\"details\", {})\n",
    "        macro = d.get(\"macro\", {}) or {}\n",
    "        acc = m.get(split, {}).get(\"accuracy\")\n",
    "        rows += [\n",
    "            {\"model\": model, \"metric\": \"accuracy\", \"value\": acc},\n",
    "            {\"model\": model, \"metric\": \"macro_precision\", \"value\": macro.get(\"precision\")},\n",
    "            {\"model\": model, \"metric\": \"macro_recall\", \"value\": macro.get(\"recall\")},\n",
    "            {\"model\": model, \"metric\": \"macro_f1\", \"value\": macro.get(\"f1\")},\n",
    "        ]\n",
    "    df = pd.DataFrame(rows).dropna(subset=[\"value\"])\n",
    "    order = [\"accuracy\", \"macro_precision\", \"macro_recall\", \"macro_f1\"]\n",
    "    df[\"metric\"] = pd.Categorical(df[\"metric\"], categories=order, ordered=True)\n",
    "    return df.sort_values([\"metric\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_df_per_class(all_metrics, split=\"test\", classes=None):\n",
    "    rows = []\n",
    "    for model, m in all_metrics.items():\n",
    "        per_cls = m.get(split, {}).get(\"details\", {}).get(\"per_class\", {}) or {}\n",
    "        keys = classes if classes is not None else list(per_cls.keys())\n",
    "        for cls in keys:\n",
    "            blk = per_cls.get(cls, {})\n",
    "            if not blk:\n",
    "                continue\n",
    "            rows += [\n",
    "                {\"model\": model, \"class\": cls, \"metric\": \"precision\", \"value\": blk.get(\"precision\")},\n",
    "                {\"model\": model, \"class\": cls, \"metric\": \"recall\",    \"value\": blk.get(\"recall\")},\n",
    "                {\"model\": model, \"class\": cls, \"metric\": \"f1\",        \"value\": blk.get(\"f1\")},\n",
    "            ]\n",
    "    df = pd.DataFrame(rows).dropna(subset=[\"value\"])\n",
    "    df[\"metric\"] = pd.Categorical(df[\"metric\"], categories=[\"precision\",\"recall\",\"f1\"], ordered=True)\n",
    "    # Optional: order classes consistently if provided\n",
    "    if classes is not None:\n",
    "        df[\"class\"] = pd.Categorical(df[\"class\"], categories=classes, ordered=True)\n",
    "    return df.sort_values([\"class\", \"metric\", \"model\"]).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553b59c-9ee8-49cb-9cc8-ec66608e9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_test = build_df_overall(all_metrics, \"test\")\n",
    "df_classes_test = build_df_per_class(all_metrics, \"test\", classes=[\"ADLs\", \"Falls\", \"Near_Falls\"])\n",
    "\n",
    "df_overall_val = build_df_overall(all_metrics, \"val\")\n",
    "df_classes_val = build_df_per_class(all_metrics, \"val\", classes=[\"ADLs\", \"Falls\", \"Near_Falls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22ddf1-9308-4e20-9cf7-cdfc1d899034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14418f41-e852-4ebb-902c-fd0bd49727fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac030ab-5b27-4f51-8329-8b9b60d24deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6260c77-9a91-4f95-82c9-39bb7901793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2720c854-3ed9-4695-b69d-f2b3b3390f62",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fbc20-0fce-477a-95d4-99ca2209b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _annotate_bars(ax):\n",
    "    import numpy as np\n",
    "    for p in ax.patches:\n",
    "        h = p.get_height()\n",
    "        if np.isfinite(h) and h != 0:\n",
    "            ax.annotate(f\"{h:.3f}\",\n",
    "                        (p.get_x() + p.get_width()/2, h),\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9, xytext=(0, 2),\n",
    "                        textcoords=\"offset points\")\n",
    "\n",
    "def plot_grouped_bars(df, x_col, group_col, y_col, title, y_max=1.0, x_label_map=None, band_groups=True):\n",
    "    # Prepare category order & groups\n",
    "    cats = list(df[x_col].cat.categories if hasattr(df[x_col], \"cat\") else sorted(df[x_col].unique()))\n",
    "    groups = sorted(df[group_col].unique())\n",
    "\n",
    "    x = np.arange(len(cats))\n",
    "    # Narrower bars\n",
    "    width = 0.55 / max(len(groups), 1)\n",
    "    group_span = width * len(groups)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Plot bars per group\n",
    "    for gi, g in enumerate(groups):\n",
    "        sub = df[df[group_col] == g]\n",
    "        y = [sub[sub[x_col] == c][y_col].values[0] if c in set(sub[x_col]) else np.nan for c in cats]\n",
    "        ax.bar(x + (gi - (len(groups)-1)/2) * width, y, width=width, label=g, zorder=1)\n",
    "\n",
    "    # Labels & legend\n",
    "    ax.set_title(title)\n",
    "    if not band_groups:\n",
    "        ax.set_ylim(0, y_max if y_max > 0 else 1.0)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([x_label_map.get(c, c) if x_label_map else c for c in cats], rotation=0)\n",
    "    leg = ax.legend(title=\"Model\", frameon=True, fancybox=True)\n",
    "    if leg is not None:\n",
    "        leg.get_frame().set_alpha(0.85)  # subtle background for the legend box\n",
    "\n",
    "    _annotate_bars(ax)\n",
    "    ax.grid(axis=\"y\", alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76a319-9370-485a-81ae-f9de55ea4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_label_map = {\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"macro_precision\": \"Macro Precision\",\n",
    "    \"macro_recall\": \"Macro Recall\",\n",
    "    \"macro_f1\": \"Macro F1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361cb1f-683b-4447-8b3c-ba60e290bbc8",
   "metadata": {},
   "source": [
    "### Plot - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c483598-0c1d-4d9f-8a86-8600e200444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_bars(\n",
    "    df_overall_test,\n",
    "    x_col=\"metric\",\n",
    "    group_col=\"model\",\n",
    "    y_col=\"value\",\n",
    "    title=\"Overall Comparison (Test)\",\n",
    "    y_max=1.05,\n",
    "    x_label_map=overall_label_map,\n",
    ")\n",
    "\n",
    "for cls in [\"ADLs\", \"Falls\", \"Near_Falls\"]:\n",
    "    df_c = df_classes_test[df_classes_test[\"class\"] == cls]\n",
    "    class_label_map = {\"precision\": \"Precision\", \"recall\": \"Recall\", \"f1\": \"F1\"}\n",
    "    plot_grouped_bars(\n",
    "        df_c,\n",
    "        x_col=\"metric\",\n",
    "        group_col=\"model\",\n",
    "        y_col=\"value\",\n",
    "        title=f\"Per-Class Comparison — {cls} (Test)\",\n",
    "        y_max=1.05,\n",
    "        x_label_map=class_label_map,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98481e6b-24d7-492e-b04a-643723099511",
   "metadata": {},
   "source": [
    "### Plot - Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7ff0d-e65a-42e9-8f3a-fad27c81a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_bars(\n",
    "    df_overall_val,\n",
    "    x_col=\"metric\",\n",
    "    group_col=\"model\",\n",
    "    y_col=\"value\",\n",
    "    title=\"Overall Comparison (Validation)\",\n",
    "    y_max=1.05,\n",
    "    x_label_map=overall_label_map,\n",
    ")\n",
    "\n",
    "for cls in [\"ADLs\", \"Falls\", \"Near_Falls\"]:\n",
    "    df_c = df_classes_val[df_classes_val[\"class\"] == cls]\n",
    "    class_label_map = {\"precision\": \"Precision\", \"recall\": \"Recall\", \"f1\": \"F1\"}\n",
    "    plot_grouped_bars(\n",
    "        df_c,\n",
    "        x_col=\"metric\",\n",
    "        group_col=\"model\",\n",
    "        y_col=\"value\",\n",
    "        title=f\"Per-Class Comparison — {cls} (Validation)\",\n",
    "        y_max=1.05,\n",
    "        x_label_map=class_label_map,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f7899-65fc-4588-bf2d-d11e9b7450f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3db7e-bca2-425f-8592-831482b2197d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fall-detection)",
   "language": "python",
   "name": "fall-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
