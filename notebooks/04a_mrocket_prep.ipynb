{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673f6fc8-7d48-4ac8-85a4-8a1f441df3ba",
   "metadata": {},
   "source": [
    "# MiniRocket - Data Preparation\n",
    "\n",
    "A bit of sanity checks on the data and prepare it into Nested Panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa42b6-7b42-47b6-8f8c-a9dec9d84407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & paths\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bc311-7a40-47d3-b715-6e51243fd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "ARTIFACTS_XGB = DATA_DIR / \"artifacts/xgb\"\n",
    "ARTIFACTS_MINI = DATA_DIR / \"artifacts/minirocket\"\n",
    "ARTIFACTS_MINI.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config (feel free to tweak)\n",
    "ID_COLS = [\"subject\", \"experiment\"]\n",
    "TARGET_COL = \"label\"\n",
    "TIME_COL = \"Time\"\n",
    "\n",
    "# Fixed-length resampling length for MiniRocket.\n",
    "# Set to None to keep native lengths (you'll need padding/windowing later).\n",
    "RESAMPLE_LEN = 256\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a93eba-23a5-4320-b9d9-f79023829b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unified dataframe\n",
    "df_path = DATA_DIR / \"consolidated\" / \"fall_dataset.parquet\"\n",
    "df = pd.read_parquet(df_path)\n",
    "\n",
    "# Defensive sort\n",
    "df = df.sort_values(ID_COLS + [TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist()[:8], \"... (+ more)\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7005d4-e5d6-43c1-a364-875b8f269d12",
   "metadata": {},
   "source": [
    "## Basic Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce0976-3fba-461f-8ad5-696eca310a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify sensor columns\n",
    "EXCLUDE = [TIME_COL] + ID_COLS + [TARGET_COL]\n",
    "SENSOR_COLS = [c for c in df.columns if c not in EXCLUDE]\n",
    "assert len(SENSOR_COLS) > 0, \"No sensor columns detected.\"\n",
    "\n",
    "print(f\"Detected {len(SENSOR_COLS)} sensor channels.\")\n",
    "print(\"Example channels:\", SENSOR_COLS[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a286721-e7c2-4366-b793-13dcb28bdd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure one label per (subject, experiment)\n",
    "lbl_counts = df.groupby(ID_COLS)[TARGET_COL].nunique()\n",
    "multi_lbl = lbl_counts[lbl_counts > 1]\n",
    "if len(multi_lbl) > 0:\n",
    "    raise ValueError(\n",
    "        f\"Found {len(multi_lbl)} experiments with multiple labels. \"\n",
    "        f\"First few: {multi_lbl.head().to_dict()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0be483-efaf-4c05-bea2-8c68fad5fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Time is monotonic inside each group (already sorted above)\n",
    "is_monotonic = (\n",
    "    df.groupby(ID_COLS)[TIME_COL].apply(lambda s: s.is_monotonic_increasing).all()\n",
    ")\n",
    "assert is_monotonic, \"Time must be monotonic per (subject, experiment).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f36085-4079-463b-b5c5-c93058e65706",
   "metadata": {},
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82712eb1-e06c-4056-938f-7efa3ea38786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build label maps\n",
    "labels_sorted = sorted(df[TARGET_COL].unique())\n",
    "label_to_id = {lbl: i for i, lbl in enumerate(labels_sorted)}\n",
    "id_to_label = {i: lbl for lbl, i in label_to_id.items()}\n",
    "print(\"Label map:\", label_to_id)\n",
    "\n",
    "# Per-experiment label_id table\n",
    "exp_labels = (\n",
    "    df.groupby(ID_COLS)[TARGET_COL]\n",
    "      .first()\n",
    "      .map(label_to_id)\n",
    "      .astype(int)\n",
    "      .rename(\"label_id\")\n",
    "      .to_frame()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Experiments total:\", len(exp_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5106aed-40d0-4289-a5ce-ccf97e63732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Derive per-subject label (majority label across that subject's experiments)\n",
    "subj_major = (\n",
    "    exp_labels.groupby(\"subject\")[\"label_id\"]\n",
    "    .agg(lambda x: x.value_counts().index[0])\n",
    "    .rename(\"subject_major_label\")\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "subjects = subj_major[\"subject\"].to_numpy()\n",
    "subj_labels = subj_major[\"subject_major_label\"].to_numpy()\n",
    "\n",
    "# First split: subjects -> (trainval_subjects, test_subjects)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "(trainval_idx, test_idx) = next(gss.split(subjects, subj_labels, groups=subjects))\n",
    "trainval_subjects = set(subjects[trainval_idx])\n",
    "test_subjects     = set(subjects[test_idx])\n",
    "\n",
    "# Second split: within trainval subjects → train vs val (again group-wise)\n",
    "subj_tv = subjects[trainval_idx]\n",
    "subj_tv_labels = subj_labels[trainval_idx]\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE+1)\n",
    "(train_idx2, val_idx2) = next(gss2.split(subj_tv, subj_tv_labels, groups=subj_tv))\n",
    "train_subjects = set(subj_tv[train_idx2])\n",
    "val_subjects   = set(subj_tv[val_idx2])\n",
    "\n",
    "print(\"Subjects per split:\",\n",
    "      \"train:\", len(train_subjects),\n",
    "      \"val:\", len(val_subjects),\n",
    "      \"test:\", len(test_subjects))\n",
    "\n",
    "# Map experiments into splits based on subject membership\n",
    "def exps_for_subjects(subj_set):\n",
    "    return (exp_labels[exp_labels[\"subject\"].isin(subj_set)][[\"subject\",\"experiment\"]]\n",
    "            .itertuples(index=False, name=None))\n",
    "\n",
    "train_idx = list(exps_for_subjects(train_subjects))\n",
    "val_idx   = list(exps_for_subjects(val_subjects))\n",
    "test_idx  = list(exps_for_subjects(test_subjects))\n",
    "\n",
    "print(\"Experiments per split:\",\n",
    "      \"train:\", len(train_idx),\n",
    "      \"val:\", len(val_idx),\n",
    "      \"test:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e593eb-b582-4e7b-b136-05956251ff34",
   "metadata": {},
   "source": [
    "## Build Labels\n",
    "\n",
    "Build labels for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d436c6-52fa-42a9-aeae-e3ced44e83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: build per-experiment label Series\n",
    "def build_labels(df: pd.DataFrame) -> pd.Series:\n",
    "    exp_lbl = (\n",
    "        df.groupby(ID_COLS)[TARGET_COL]\n",
    "        .first()\n",
    "        .map(label_to_id)\n",
    "        .astype(int)\n",
    "        .rename(\"label_id\")\n",
    "    )\n",
    "    exp_lbl.index = pd.MultiIndex.from_tuples(exp_lbl.index, names=ID_COLS)\n",
    "    return exp_lbl\n",
    "\n",
    "labels_all = build_labels(df)\n",
    "labels_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6a90a-69f1-449d-b4c5-5d46020aaa28",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ee413-f9e7-445c-bc5f-a0553cec4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Time to numeric seconds relative to experiment start (robust for datetime or float)\n",
    "def _to_seconds_relative(s: pd.Series) -> np.ndarray:\n",
    "    # If datetime-like, convert; else assume numeric\n",
    "    if np.issubdtype(s.dtype, np.datetime64):\n",
    "        s = s.view(\"int64\") / 1e9  # ns -> s\n",
    "    return (s - s.iloc[0]).astype(float).to_numpy()\n",
    "\n",
    "# Helper: resample a single experiment into fixed-length multivariate series\n",
    "def resample_experiment(exp_df: pd.DataFrame, resample_len: int) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dict[channel_name] = pd.Series(length=resample_len, index=range(resample_len))\n",
    "    Uses linear interpolation over normalized time grid [0, 1].\n",
    "    \"\"\"\n",
    "    # Time to normalized [0, 1]\n",
    "    t_sec = _to_seconds_relative(exp_df[TIME_COL])\n",
    "    if t_sec[-1] == 0:\n",
    "        # Degenerate case: single timestamp -> replicate values\n",
    "        t_norm = np.zeros_like(t_sec)\n",
    "        grid = np.zeros(resample_len)\n",
    "    else:\n",
    "        t_norm = t_sec / t_sec[-1]\n",
    "        grid = np.linspace(0.0, 1.0, resample_len, endpoint=True)\n",
    "\n",
    "    out = {}\n",
    "    for ch in SENSOR_COLS:\n",
    "        y = exp_df[ch].to_numpy(dtype=float)\n",
    "        # Handle NaNs: forward-fill then zeros as last resort\n",
    "        if np.isnan(y).any():\n",
    "            s = pd.Series(y).ffill().bfill().fillna(0.0).to_numpy()\n",
    "        else:\n",
    "            s = y\n",
    "        # Interpolate to uniform grid\n",
    "        if len(s) == 1:\n",
    "            yi = np.full(resample_len, s[0], dtype=float)\n",
    "        else:\n",
    "            yi = np.interp(grid, t_norm, s)\n",
    "        out[ch] = pd.Series(yi, index=pd.RangeIndex(resample_len, name=\"t\"))\n",
    "    return out\n",
    "\n",
    "# Helper: pack panel into sktime nested DataFrame\n",
    "def pack_nested(panels: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    panels: list of dicts [{channel: pd.Series(...), ...}, ...] in the same channel order.\n",
    "    Returns a nested DataFrame with columns=channels, n_rows=len(panels).\n",
    "    \"\"\"\n",
    "    channels = list(panels[0].keys())\n",
    "    data = {ch: [p[ch] for p in panels] for ch in channels}\n",
    "    nested = pd.DataFrame(data)\n",
    "    return nested\n",
    "\n",
    "# Build nested panels per split\n",
    "def build_split_nested(df_all: pd.DataFrame, idx: pd.MultiIndex, resample_len: int):\n",
    "    # Slice only the groups in the split\n",
    "    panels = []\n",
    "    y_list = []\n",
    "    missing_groups = []\n",
    "    # Iterate groups by (subject, experiment)\n",
    "    grouped = df_all.groupby(ID_COLS, sort=False)\n",
    "    want = set(idx)  # fast membership\n",
    "    for key, g in grouped:\n",
    "        if key not in want:\n",
    "            continue\n",
    "        # Prepare label\n",
    "        lbl = g[TARGET_COL].iloc[0]\n",
    "        y_list.append(label_to_id[lbl])\n",
    "        # Panelize\n",
    "        if resample_len is not None:\n",
    "            panel = resample_experiment(g, resample_len=resample_len)\n",
    "        else:\n",
    "            # Keep native length: use direct values as a Series indexed by 0..T-1\n",
    "            # (You must pad/window later before MiniRocket.)\n",
    "            t_len = len(g)\n",
    "            out = {}\n",
    "            for ch in SENSOR_COLS:\n",
    "                y = g[ch].to_numpy(dtype=float)\n",
    "                if np.isnan(y).any():\n",
    "                    y = pd.Series(y).ffill().bfill().fillna(0.0).to_numpy()\n",
    "                out[ch] = pd.Series(y, index=pd.RangeIndex(t_len, name=\"t\"))\n",
    "            panel = out\n",
    "        panels.append(panel)\n",
    "\n",
    "    # Pack into nested frame (channels as columns)\n",
    "    nested = pack_nested(panels)\n",
    "    y_ser = pd.Series(y_list, name=\"label_id\").astype(int)\n",
    "    # Row index mirrors input order of idx; we can reindex to match that order\n",
    "    # Build a map key->row for deterministic ordering\n",
    "    # For simplicity, align to the iteration order of idx:\n",
    "    # rebuild panels/y in idx order\n",
    "    key_to_pos = {k: i for i, k in enumerate(set(df_all.groupby(ID_COLS).groups.keys()))}\n",
    "\n",
    "    # Better: just ensure ordering by idx explicitly\n",
    "    # Build list again, ordered by idx\n",
    "    ordered_panels = []\n",
    "    ordered_y = []\n",
    "    groups = dict(tuple(df_all.groupby(ID_COLS, sort=False)))\n",
    "    for key in idx:\n",
    "        g = groups.get(key)\n",
    "        if g is None:\n",
    "            missing_groups.append(key)\n",
    "            continue\n",
    "        lbl = g[TARGET_COL].iloc[0]\n",
    "        if resample_len is not None:\n",
    "            panel = resample_experiment(g, resample_len=resample_len)\n",
    "        else:\n",
    "            t_len = len(g)\n",
    "            out = {}\n",
    "            for ch in SENSOR_COLS:\n",
    "                yv = g[ch].to_numpy(dtype=float)\n",
    "                if np.isnan(yv).any():\n",
    "                    yv = pd.Series(yv).ffill().bfill().fillna(0.0).to_numpy()\n",
    "                out[ch] = pd.Series(yv, index=pd.RangeIndex(t_len, name=\"t\"))\n",
    "            panel = out\n",
    "        ordered_panels.append(panel)\n",
    "        ordered_y.append(label_to_id[lbl])\n",
    "\n",
    "    nested = pack_nested(ordered_panels)\n",
    "    y_ser = pd.Series(ordered_y, name=\"label_id\").astype(int)\n",
    "    return nested, y_ser\n",
    "\n",
    "print(\"Building nested panels (this may take a minute depending on data size)...\")\n",
    "train_nested, y_train = build_split_nested(df, train_idx, RESAMPLE_LEN)\n",
    "val_nested,   y_val   = build_split_nested(df, val_idx,   RESAMPLE_LEN)\n",
    "test_nested,  y_test  = build_split_nested(df, test_idx,  RESAMPLE_LEN)\n",
    "\n",
    "print(\"Nested shapes (rows, channels):\")\n",
    "print(\"  train:\", train_nested.shape, \"  val:\", val_nested.shape, \"  test:\", test_nested.shape)\n",
    "print(\"Each cell is a pd.Series of length:\", RESAMPLE_LEN if RESAMPLE_LEN else \"(native length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cf33e-7e27-4c07-9d5c-52044919898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c19886-ddd3-49ba-ad38-005a3da60fc7",
   "metadata": {},
   "source": [
    "## Persist  Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6ca4f-33bf-4dc0-afd8-bd10a811712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..\n",
    "(train_nested).to_pickle(ARTIFACTS_MINI / \"train_nested.pkl\")\n",
    "(val_nested).to_pickle(ARTIFACTS_MINI / \"val_nested.pkl\")\n",
    "(test_nested).to_pickle(ARTIFACTS_MINI / \"test_nested.pkl\")\n",
    "\n",
    "y_train.to_frame().to_parquet(ARTIFACTS_MINI / \"train_y.parquet\", index=False)\n",
    "y_val.to_frame().to_parquet(ARTIFACTS_MINI / \"val_y.parquet\", index=False)\n",
    "y_test.to_frame().to_parquet(ARTIFACTS_MINI / \"test_y.parquet\", index=False)\n",
    "\n",
    "# Splits file\n",
    "splits = {\n",
    "    \"label_to_id\": label_to_id,\n",
    "    \"id_to_label\": {int(k): v for k, v in id_to_label.items()},\n",
    "    \"train_idx\": train_idx,\n",
    "    \"val_idx\": val_idx,\n",
    "    \"test_idx\": test_idx,\n",
    "    \"split_strategy\": {\n",
    "        \"group\": \"subject\",\n",
    "        \"method\": \"GroupShuffleSplit(train/val/test)\",\n",
    "        \"test_size_subjects\": 0.2,\n",
    "        \"val_size_subjects_in_trainval\": 0.2,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    },\n",
    "}\n",
    "with open(ARTIFACTS_MINI / \"splits.json\", \"w\") as f:\n",
    "    json.dump(splits, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Manifest for the modeling notebook\n",
    "manifest = {\n",
    "    \"version\": 1,\n",
    "    \"time_col\": TIME_COL,\n",
    "    \"id_cols\": ID_COLS,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"sensor_channels\": SENSOR_COLS,\n",
    "    \"resample_len\": RESAMPLE_LEN,\n",
    "    \"paths\": {\n",
    "        \"train_nested\": str(ARTIFACTS_MINI / \"train_nested.pkl\"),\n",
    "        \"val_nested\": str(ARTIFACTS_MINI / \"val_nested.pkl\"),\n",
    "        \"test_nested\": str(ARTIFACTS_MINI / \"test_nested.pkl\"),\n",
    "        \"train_y\": str(ARTIFACTS_MINI / \"train_y.parquet\"),\n",
    "        \"val_y\": str(ARTIFACTS_MINI / \"val_y.parquet\"),\n",
    "        \"test_y\": str(ARTIFACTS_MINI / \"test_y.parquet\"),\n",
    "        \"splits\": str(ARTIFACTS_MINI / \"splits.json\"),\n",
    "    },\n",
    "}\n",
    "with open(ARTIFACTS_MINI / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n=== MiniRocket DATA PREP — DONE ===\")\n",
    "print(\"Artifacts saved in:\", ARTIFACTS_MINI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5e6b4-2755-4a30-9e17-5d4886beaaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57564a50-5688-423d-b905-fc0402b70220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d129f-8c99-4cc4-9500-52701ffe45cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0eb573-6541-4131-aeb0-8e38dd7614ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff831e96-5d31-4417-8ed4-092be2faefec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fde58a-8705-45a0-91eb-656b35806e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fall-detection)",
   "language": "python",
   "name": "fall-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
